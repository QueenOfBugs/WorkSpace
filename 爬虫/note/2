[TOC]
# 爬虫基础

## 爬虫概述

### 1. 爬虫的概念

**模拟浏览器，发送请求，获取响应**

### 2. 爬虫的作用
数据采集</br>
软件测试</br>
抢票</br>
网站上的投票</br>
网络安全</br>

### 3. 爬虫的分类

#### 3.1 根据爬取网站的数量

- 通用爬虫： 如搜索引擎
- 聚焦爬虫：如12306抢票，或者专门爬取某一类(个)网站数据

#### 3.2 是否以获取数据为目的

- 功能性爬虫：投票，点赞
- 数据增量型爬虫：如招聘信息

#### 3.3 根据url地址变化和对应页面内容是否改变，数据增量爬虫可以细分
- 基于url地址变化，内容也变化的数据增量爬虫
- url地址不变，内容变化的数据增量型爬虫

### 4. 爬虫的流程
url</br>
发送请求，获取响应</br>
解析</br>

## http协议复习
### 1. http以及https的概念和区别
> HTTPS比HTTP更安全，但是性能更低

- HTTP: 超文本传输协议 默认端口80
	- 超文本： 超过文本，不仅限于文本，还包括图片、音频、视频等文件。
	- 传书协议：用固定格式来传递转换成字符串的超文本内容.
- HTTPS: HTTP+SSL(安全套接字）就是带有安全套接字协议的超文本传输协议 默认端口443
	- SSL对传输的内容进行加密
- 可以打开浏览器访问一个url，右键检查，点击net work,点选一个url,查看http协议的形式

### 2. 常见的请求头和响应头

请求头</br>
```
host	域名

Connection	长连接

Upgrade-Insecure-Requests	升级为HTTPS连接

* User-Agent	User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:78.0) Gecko/20100101 Firefox/78.0 
用户代理，提供系统信息和浏览器信息

* Referer		页面跳转处，防盗链(这次访问是从哪里过来的)

* Cookie 		状态保持

Authorization(自己做测试的时候，用于认证的信息)
```
响应头

```
Set-Cookie	(对方服务器设置cookie到用户浏览器的缓存)

```

### 3. 常见的响应状态码

- 200: 成功
- 302: 跳转，新的url在响应的Location头中给出
- 303: 浏览器对于POST的响应进行重定向至新的url
- 307: 浏览器对于GET的响应重定向至新的url
- 403: 资源不可用;服务器理解客户的请求，但拒绝处理它(没有权限)
- 404: 找不到该页面
- 500: 服务器内部错误
- 503: 服务器由于维护或者负载过重未能应答，在响应中可能会携带Retry-After响应头;有可能是因为爬虫频繁访问url,使服务器忽略爬虫的请求,最终返回503响应状态码

### 4.状态码
**所有状态码都不可信，一切以是否从抓包得到的响应中获取到数据为准**

network中抓包得到的源码才是判断依据，elements中的源码是渲染之后的源码，不能作为判断标准

### 5.浏览器请求的过程
```
浏览器
	发送所有请求，进行渲染
爬虫
	只发送指定请求，不会渲染
骨骼文件
	html静态文件
肌肉文件
	js/ajax请求
皮肤
	CSS/Font/图片
抓包过程：
	根据发送请求的流程分别在骨骼/肌肉/皮肤响应中查找数据 
```
**爬虫是不会进行渲染的，所以需要进行抓包查找数据**

### 6.HTTP协议的其他参考阅读

* https://blog.csdn.net/qq_33301113/article/category/6943422/2
* https://www.xuebuyuan.com/3252125.html
* https://baike.baidu.com/item/http/243074?fr=aladdin
* https://www.janshu.com/p/cc1fea7810b2
* https://blog.csdn.net/qq_30553235/article/details/79282113
* https://segmentfault.com/1/1010000002403462


# resquests模块

> requests 是http模块，用于发送请求获取响应，有替代模块urllib模块，但是工作中用的最多的还是requests模块，requests 模块代码简介易懂，用requests编写的爬虫代码会更少，功能实现起来更简单

## requests模块

知识点:
- 掌握 headers参数的使用
- 掌握 发送带参数的请求
- 掌握 headers中携带cookie
- 掌握 cookies参数的使用
- 掌握 cookieJar的转换方法
- 掌握 超时参数timeout的使用
- 掌握 代理ip参数proxies的使用
- 掌握 使用verify参数忽略CA整数
- 掌握 requests模块发送POST请求
- 掌握 利用requests.session进行状态保持

### 1.requests模块介绍

> requests文档

#### 1.1 requestes模块作用：

* 发送http请求，获取响应数据

#### 1.2 requests 是第三方模块，需要在python(虚拟)环境中额外安装

* pip/pip3 install requests

#### 1.3 requests模块发送get请求

> 1.需求：通过requests向百度发送请求，获取页面的源码
> 2.运行下面代码，观察打印输出的结果

```python
# 代码实现
##导入requests模块
import requests

##设置url
url = 'https://www.baidu.com'

## 发送get请求，获取响应
response = requests.get(url)

## 打印响应内容
print(resposne.text)
 
```


### 2.response响应对象

> 上面的代码发现打印出来的好多乱码，这是因为编解码使用了不同的字符集

#### 2.1 response.text 和response.content的区别

- requests.text
	- 类型: str
	- 解码类型： requests模块自动根据http头部对响应的编码作出有根据的推测，推测出的文本编码
- response.content
    	- 类型:bytes
	- 解码类型:没有指定

```python
##导入requests模块
import requests

##设置url
url = 'https://www.baidu.com'

## 发送get请求，获取响应
response = requests.get(url)

## 打印响应内容
#print(resposne.text)
#这种直接打印方式是按照推测出的编码格式打印
# 手动设置response的编码格式,解决编码问题
#response.encoding = 'utf8'
#print(response.text)

```

#### 2.2 通过对response.content进行decode来解决中文乱码

- response.content.decode()	默认utf-8
- response.content.decode('GBK')
- 常见的编码字符集
	- utf-8
	- gbk
	- gb2312
	- ascii
	- iso-8859-1

```python
## 使用response.content
print(response.content.decode('utf8')
```

#### 2.3 response响应对象的其他常用 属性或方法

> response = requests.get(url)中response是发送请求的响应对象；response响应对象中除了text、content获取响应内容以外还有其他常用的属性或方法：

- response.url 响应的url
- response.status_code 响应状态码
- response.request.headers 响应对应的请求头
- response.header 响应头
- response.request_cookies 响应对象请求的cookie;返回coolkieJar类型
- response.cookies 响应的cookie(经过了set-cookie动作；返回cookieJar类型)
- response.json() 自动将json字符串类型的响应内容转换为python对象(dict or list)

```python
# response的其他属性
import requests

# 目标url
url = 'https://www.baidu.com'

# 向目标url发送get请求，并获取响应存储到response
response = requests.get(url)

print(response.url)	#打印响应的url

print(response.status_code)	#打印响应的状态码

print(response.request.headers)		#打印响应对象的请求头

print(response.headers)		#打印响应头

print(response.request._cookies)     # 打印请求携带的cookies

print(response.cookies)     # 打印响应中携带的cookies

```

### 3.requests模块发送请求

#### 3.1 发送带header的请求

> 先写一个获取百度首页的代码

```python

import requests

url = 'https://www.baidu.com'

response = requests.get(url)

print(response.content.decode())

print(response.request.headers)

```

##### 3.1.1 思考
1. 对比浏览器上百度首页的网页源代码和上面代码中获取的百度首页的源码，有什么不同？

    - 查看网页源码的方式：
        - 右键-查看网页源代码
        - 右键-检查
        - <kbd>F12</kbd>
2. 对比对应url的响应内容和代码获取的百度首页的源码，有什么不同？

    -   查看对应url响应内容的方法：
        1. 右键检查
        2. 点击net-work
        3. 勾选preserve log
        4. 刷新页面
        5. 查看Name一栏下和浏览器地址相同url的Response

3. 代码中的百度首页的源码非常少，为什么？

    - 需要我们带上请求头信息
> 爬虫要模拟浏览器，欺骗服务器，获取和浏览器一致的内容
    - 请求头中有很多字段，其中User-Agent字段必不可少，表示客户端的操作系统以及浏览器的信息

##### 3.1.2 携带请求头发送请求的方法

    requests.get(url, headers=headers)
    - headers参数接受字典形式的请求头
    - 请求头字段名为key,字段值为value

##### 3.1.3 完成代码实现

> 从浏览器中复制User-Agent,构造headers字段，完下面的代码

```python
import requests

url = 'https://www.baidu.com'

headers = {'User-Agent':'Mozilla/5.0 (X11; Linux x86_64; rv:79.0) Gecko/20100101 Firefox/79.0'}

response = requests.get(url,headers=headers)

print(response.content.decode())

```

#### 3.2 发送带参数的请求

#### 3.2.1 直接在url中携带参数

```python
...

url='https://www.baidu.com/s?wd=python'

...

```

##### 3.2.2 使用params参数

```python

import requests

url = 'https://www.baidu.com/s?'

parameters = {'wd':'python'}

response = requests.get(url,params=parameters)

print(response.content.decode())
```

#### 3.3 headers中携带cookies参数

> 网站经常利用请求头中的Cookie字段来做用户访问状态的保持,在headers参数中添加Cookie,可以模拟普通用户的请求

##### 3.3.1 github 抓包分析

1. 打开浏览器，右键检查，点击network，勾选preserve log保存日志
2. 访问github的登录url地址： https://github.com/login
3. 输入帐号密码之后，访问一个需要登录后才能获取正确内容的url，比如Your Profile,访问https://github.com/USER_NAME
4. 确定url后，再确定发送请求所需要的请求头信息中的User-Agent和Cookie

##### 3.3.2 完成代码

- 从浏览器中复制User-Agent和Cookie
- 浏览器中的请求头字段和值与headers参数必须一直
- headers请求参数字典中Cookie键对应的值是字符串

```python

import requests

url = 'https://github.com'

headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:79.0) Gecko/20100101 Firefox/79.0',
    'Cookie': '_octo=GH1.1.1310319776.1585638523; logged_in=yes; _ga=GA1.2.150556382.1585657024; experiment:homepage_signup_flow=eyJ2ZXJzaW9uIjoiMSIsInJvbGxPdXRQbGFjZW1lbnQiOjU1LjIwNDQ4OTI1NzkzMjc2LCJzdWJncm91cCI6bnVsbCwiY3JlYXRlZEF0IjoiMjAyMC0wMy0zMVQxMjoxNzowNC40MjRaIiwidXBkYXRlZEF0IjoiMjAyMC0wMy0zMVQxMjoxNzowNC40MjRaIn0=; _device_id=678adb1e5fe193f4925289600b3880ac; user_session=cosND-005AomKEF76jOvy4OtFWK0mk2tgd83WtDqTWvRqmZS; __Host-user_session_same_site=cosND-005AomKEF76jOvy4OtFWK0mk2tgd83WtDqTWvRqmZS; dotcom_user=QueenOfBugs; _gh_sess=r%2Bz6gc4fN2C1h6NMqsNLo7VR4r7rlrlymIDBEav7EuViZI%2BaJfmi5uAqjm03zHp9vttSstolaqyC5PMr4dZ2HD99J4qzwCl47oVzzcQJSyDjoxB2u7uoe2TaiFaBtajEisQwlNtItNmlShxb4AMQdkhmJ%2FwPujEKQHP1%2BJVogb6Q94UKbimlbqpsjT5ynxRgl7s%2FNl3EL3jJNeEeSkshjZdPWnvtQyIC%2BRQT1PNVJNnZ6zeoUl9qfWaSX%2BJfNpztarm4VS3oN2bF83euofPAlK93X4%2FFl1vUDx8C7at1DzsTSpXM4hCCF9lYVzK65yMdPboMPPEyxuHl1kJBq9pBq6diXOyJ%2Bo%2F4eAyRRWa1PafBgOYmoFA4Qqj7czUckrpGm4Zafw%2FFMNW7KKhG2%2BMypRx9vq9mOVsIbmoPScRnUsp%2FhBRBRTgzG66HhXSaeT7MQKsjTby0iGBM7wmJT6%2BrzE0JRr1MELVayojOfI1YjmrBRTAOasoP6SQ8fQ%2B1L%2BAUFa9ItRIWA2K%2BBnzygcIGCufiRKJB7J9ePOAtFFWMJxJlkx6dvTbI%2Bn4Gg4RIhTa4KGUjdoy%2Fj6Mg%2Fj9bnh7pt91wxvXhdp4oBHXd5jvXk2NsSehdhlSuHg0XZZlB9srWLTcRZvqydKWWmkFpFZepIwtFKGsn%2FkYB7hR8M6iO4DCIwSpOXAPhj2hfIBZ%2FGu8bcAbNXqEqeAkQ0XSuIO65AfSEdHUcQ%2BF8iX0FaPwwfkVxNvSwsdZ8elVQ%2Bkc5GLjF3%2F3BxnIGsc5%2B6%2BzifWitMyYHHMhCsFgM0ldIW%2BsKrAZqxEmf9qfX%2B3UfvgkmDsXRsybpOY4pXlcTIineVHPOIOm%2Bg%2FR%2FeX0coFQsDGeZdbIB28D8HpZNFZ1MRN3p9x4NU5GHKIy285rmiIhwkzWhBwRWRBZdIoFTwsPXGM4mQuH5adjJ4x2x4zxZ5QeigDo2sG28EsG17h9EGtRwRdo89CGKk%2BcOhY%2BIH0iiQC3Bb%2FJp8Tn%2FZ0DfPS%2F%2B%2F49ZI1pw%2FEaCa9LZCc4%3D--l3LEYFfPQ9oohyYb--BrKWmWJ7tu39qr8o2dHsng%3D%3D; tz=Asia%2FShanghai; has_recent_activity=1'}

headers_without_cookie = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:79.0) Gecko/20100101 Firefox/79.0'}

response = requests.get(url, headers = headers)

response2 = requests.get(url, headers=headers_without_cookie)

with open('github_cookies.html', 'wb') as f:
    f.write(response.content)
    f.close()

with open('github_without_cookie.html', 'wb') as f:
    f.write(response2.content)
    f.close()


```

##### 3.3.3 验证结果

> 上面的代码中将带cookie和不带cookie获取的页面存入了两个文件，可以用浏览器打开查看结果


#### 3.4 cookies参数的使用

> 除了在headers参数中携带cookie外，也可以使用专门的cookie参数

1. cookies参数的形式：字典
```python
cookies = {'Cookie': 'value'}
    ```
    - 字典对应请求头中的Cookie字符串，以分号、空格分割每一对字典键值对
    - 等号左边的是一个cookie的名字，对应cookies字典的key
    - 等号右边对应cookies字典的value
2. cookies参数的使用方法
```python
response = requests.get(url, headers=headers, cookies=cookies)
```
3. 将cookie字符串转换为cookies参数所需要的字典
```python
cookies_dict = {cookie.split('=')[0]:cookie.split('=')[-1] for cookie in cookies_str.split('; ')}
```
4. cookie一般是有过期时间的，一旦过期需要重新获取


#### 3.5 cookieJar对象转换为cookies字典
> 使用requests获取的response对象，具有cookies属性，该属性是一个cookieJar类型，包含了对方服务器设置在本地的cookie.

1. 转换方法
```python
cookies_dict = requests.utils.dict_from_cookiejar(response.cookies)
```
2. 其中response.cookies返回的就是cookieJar类型的对象
3. requests.utils.dict_from_cookiejar函数返回cookies字典

#### 3.6 超时参数timeout的使用
> 上网时因为网络波动，一个请求会等了很久没有结果，在爬虫中，一个请求很久没有结果会使得整个项目的效率降低，这时我们就要对请求进行强制要求，让他在特定时间内返回结果，否则就报错

1. 超时参数timeout的使用方法
```python
response = requests.get(url,timeout=3)
```
2. timeout=3 表示发送请求后,3s内返回响应，否则就抛出异常
```python
import requests
url = ''
response = requests.get(url,timeout=3)  #设置超时时间
```

#### 3.7 了解代理以及代理参数proxy的使用

##### 3.7.1 理解使用代理的过程
1. 代理是一个ip，指向的是一个代理服务器
2. 代理服务器能够帮我们向目标服务器转发请求

##### 3.7.2 正向代理和反向代理的区别
> 前面提到proxy参数指定的代理ip指向的是正向的代理服务器，那么相应的就有反向服务器

1. 从发送请求一方的角度来区分正向和反向
2. 为浏览器或客户端(发送请求端)转发请求的，叫正向代理
    - 浏览器知道最终处理请求的服务器的真实ip地址，如vpn
3. 不为浏览器或客户端转发请求，而是为服务器转发请求的，叫做反向代理
    - 浏览器不知道服务器的真实地址，如nginx

##### 3.7.3 代理ip(代理服务器)的分类

1. 根据代理ip的匿名程度划分：
    - 透明代理(Transparent Proxy): 透明代理虽然可以直接"隐藏"你的IP地址，但是还是可以查到你是谁，目标服务器收到的请求头：
        ```python
            REMOTE_ADDR = Proxy IP
            HTTP_VIA = Ptoxy IP
            HTTP_X_FORMARDED_FOR = Your IP
        ```
    - 匿名代理(Anonymous Proxy): 使用匿名代理，别人只能知道你用了代理，但无法知道你是谁，目标服务器收到的请求头如下：
        ```python
            REMOTE_ADDR = proxy IP
            HTTP_VIA = proxy IP
            HTTP_X_FORMARDED_FOR = proxy IP
        ```

    - 高匿代理(Elite proxy或High Anonymity Proxy):高匿代理让别人无法发现你是否是代理，这是使用爬虫最好的选择，毫无疑问使用高匿代理效果最好，目标服务器收到的请求头：
        ```python
            REMOTR_ADDR = proxy IP
            HTTP_VIA = not determined
            HTTP_X_FORMARDED_FOR = proxy IP
        ```
2. 根据网站使用的协议不同，需要使用相应协议的代理服务，从代理服务请求使用的协议来分
    - http代理: 目标url为http协议
    - https代理: 目标url为https协议
    - socks隧道代理(如socks5代理)等:
        - socks 代理只是简单的传递数据包，不关心使用的是何种应用协议
        - socks 代理比http、https代理耗时少
        - socks 代理可以转发http、https的请求

##### 3.7.4 proxies代理参数的使用
> 为了让服务器以为不是同一个客户端在请求，为了防止频繁向同一个域名发送请求被封ip，所以我们需要使用代理ip

- 用法：
```python
response = requests.get(url,proxies=proxies)
```

- proxies的形式：字典

- 例如：
```python
proxies = {
    "http": "http://12.34.56.79:9527",
    "https": "https://12.34.56.79:9527",
}
```

- 注意： 如果proxies字典中包含多个键值对，发送请求时将按照url地址的协议来选择相应的代理ip

> 在网上可以找到一些免费的代理

#### 3.8 使用verify 参数忽略CA证书
> 在使用浏览器上网时，有时能看到"您的连接不是私密连接"的提示

有些网站的CA证书不是官方下发(认证)的，这时就要使用verify参数忽略CA证书

##### 3.8.1 运行代码查看代码中向不安全的链接发起请求的效果
> 运行代码会抛出ssl.CertificateErrot...异常

```python
import requests
url = 'https://sam.huat.edu.cn:8443/selfservice'
response = requests.get(url, verify=False)
```

#### 4. requests模块发送post请求
> 那些地方会用到POST请求？
> 1. 登录注册(在web工程师看来POST比GET更安全，url地址中不会暴露用户的帐号密码等信息)
> 2. 需要传书打文本内容的时候(POST请求对数据长度没有要求)
> 所以同样的，爬虫也许要在这两个地方去模拟浏览器发送post请求

##### 4.1 requests发送post请求的方法
- response = requests.post(url, data)
- data 参数接受一个字典
- requests模块发送post请求函数的其他参数和发送get请求的参数完全一致

##### 4.2 POST请求练习
<p>通过金山翻译的例子来看看post请求如何使用</p>
1. 地址： http://fy.iciba.com/

__思路分析__
1. 抓包确定请求的url
2. 确定请求的参数
3. 确定返回数据的位置
4. 模拟浏览器获取数据

##### 4.3 抓包分析的结论
1. url地址：    http://fy.iciba.com
2. 请求方法： POST
3. 请求所需参数：
```python
data = {
    'f': 'auto', #表示被翻译的语言是自动识别
    't': 'auto', #表示翻译后的语言是自动识别
    'w': '人生苦短' #表示要翻译的中文字符串
}
```
4. pc端User-Agent
```python
Mozilla/5.0 (X11; Linux x86_64; rv:79.0) Gecko/20100101 Firefox/79.0
```

##### 4.4 代码实现
> 了解requests模块发送post请求的方法，以及分析过移动端的百度翻译之后，我们来完成代码
```python
#coding:utf-8
import sys
import requests
import json

class Trans(object):

    def __init__(self, word):
        self.url = 'http://fy.iciba.com/ajax.php?a=fy'
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:79.0) Gecko/20100101 Firefox/79.0'
        }
        self.data = {
            'f': 'auto',
            't': 'auto',
            'w': word
        }
    #   get data from translation site
    def get_data(self):
        response = requests.post(self.url, headers=self.headers, data=self.data, )
        return response.content
    def parse_data(self, data):
        #loads() transform json data to dict data
        dict_data = json.loads(data)
        return  dict_data

    def run(self):
        response = self.get_data()

        # print(response.decode())
        # print(self.parse_data(response))

        trans_result_dict = self.parse_data(response)

        # print(trans_result_dict)

        try:
            trans_result = trans_result_dict['content']['out']
        except:
            trans_result = trans_result_dict['content']['word_mean']
        print(trans_result)

if __name__ == '__main__':

    word = sys.argv[1]
    # print(sys.argv)
    King = Trans(word)
    King.run()

```

##### 4.5 POST数据来源
1. 固定值   抓包比较 不变值
2. 输入值   找包比较 根据自身变化值
3. 预设值-静态文件  需要提前从静态html中获取(正则)
4. 预设值-发请求    需要对指定地址发送请求
5. 在客户端(浏览器)生成的   (javascript知识)分析js，模拟生成数据

> 1/2 简单 3/4 抓包分析 5 最难

#### 5 利用requests.session进行状态保持
> requests 模块中的Session类能够自动处理发送请求获取相应过程中产生的cookie，进而达到状态保持的目的。

##### 5.1 requests.session的作用以及应用场景

- requests.session的作用
    - 自动处理cookie,即下一次请求会带上前一次的cookie
- requests.session的场景应用
    - 自动处理连续的多次请求过程中产生的cookie

##### 5.2 requests.session使用方法
> session实例在请求了一个网站后，对方服务器设置在本地的cookie会保存在session中，下一次再使用session请求对方服务器的时候，会带上前一次的cookie

```python
session = requests.session()    #实例化session对象
response = session.get(url, headers, ...)
response = session.post(url, data, ...)
```

- session对象发送get/post请求的参数，与requests模块发送请求的参数完全一致

##### 5.3 测试
> 使用requests.session来登录github,然后获取需要登录后才能访问的页面


### 数据提取

知识点
- 了解响应内容的分类
- 了解xml和html的区别


