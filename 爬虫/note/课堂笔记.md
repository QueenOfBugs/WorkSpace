[TOC]
# 爬虫基础

## 爬虫概述

### 1. 爬虫的概念

**模拟浏览器，发送请求，获取响应**

### 2. 爬虫的作用
数据采集</br>
软件测试</br>
抢票</br>
网站上的投票</br>
网络安全</br>

### 3. 爬虫的分类

#### 3.1 根据爬取网站的数量

- 通用爬虫： 如搜索引擎
- 聚焦爬虫：如12306抢票，或者专门爬取某一类(个)网站数据

#### 3.2 是否以获取数据为目的

- 功能性爬虫：投票，点赞
- 数据增量型爬虫：如招聘信息

#### 3.3 根据url地址变化和对应页面内容是否改变，数据增量爬虫可以细分
- 基于url地址变化，内容也变化的数据增量爬虫
- url地址不变，内容变化的数据增量型爬虫

### 4. 爬虫的流程
url</br>
发送请求，获取响应</br>
解析</br>

## http协议复习
### 1. http以及https的概念和区别
> HTTPS比HTTP更安全，但是性能更低

- HTTP: 超文本传输协议 默认端口80
	- 超文本： 超过文本，不仅限于文本，还包括图片、音频、视频等文件。
	- 传书协议：用固定格式来传递转换成字符串的超文本内容.
- HTTPS: HTTP+SSL(安全套接字）就是带有安全套接字协议的超文本传输协议 默认端口443
	- SSL对传输的内容进行加密
- 可以打开浏览器访问一个url，右键检查，点击net work,点选一个url,查看http协议的形式

### 2. 常见的请求头和响应头

请求头</br>
```
host	域名

Connection	长连接

Upgrade-Insecure-Requests	升级为HTTPS连接

* User-Agent	User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:78.0) Gecko/20100101 Firefox/78.0 
用户代理，提供系统信息和浏览器信息

* Referer		页面跳转处，防盗链(这次访问是从哪里过来的)

* Cookie 		状态保持

Authorization(自己做测试的时候，用于认证的信息)
```
响应头

```
Set-Cookie	(对方服务器设置cookie到用户浏览器的缓存)

```

### 3. 常见的响应状态码

- 200: 成功
- 302: 跳转，新的url在响应的Location头中给出
- 303: 浏览器对于POST的响应进行重定向至新的url
- 307: 浏览器对于GET的响应重定向至新的url
- 403: 资源不可用;服务器理解客户的请求，但拒绝处理它(没有权限)
- 404: 找不到该页面
- 500: 服务器内部错误
- 503: 服务器由于维护或者负载过重未能应答，在响应中可能会携带Retry-After响应头;有可能是因为爬虫频繁访问url,使服务器忽略爬虫的请求,最终返回503响应状态码

### 4.状态码
**所有状态码都不可信，一切以是否从抓包得到的响应中获取到数据为准**

network中抓包得到的源码才是判断依据，elements中的源码是渲染之后的源码，不能作为判断标准

### 5.浏览器请求的过程
```
浏览器
	发送所有请求，进行渲染
爬虫
	只发送指定请求，不会渲染
骨骼文件
	html静态文件
肌肉文件
	js/ajax请求
皮肤
	CSS/Font/图片
抓包过程：
	根据发送请求的流程分别在骨骼/肌肉/皮肤响应中查找数据 
```
**爬虫是不会进行渲染的，所以需要进行抓包查找数据**

### 6.HTTP协议的其他参考阅读

* https://blog.csdn.net/qq_33301113/article/category/6943422/2
* https://www.xuebuyuan.com/3252125.html
* https://baike.baidu.com/item/http/243074?fr=aladdin
* https://www.janshu.com/p/cc1fea7810b2
* https://blog.csdn.net/qq_30553235/article/details/79282113
* https://segmentfault.com/1/1010000002403462


# resquests模块

> requests 是http模块，用于发送请求获取响应，有替代模块urllib模块，但是工作中用的最多的还是requests模块，requests 模块代码简介易懂，用requests编写的爬虫代码会更少，功能实现起来更简单

## requests模块

知识点:
- 掌握 headers参数的使用
- 掌握 发送带参数的请求
- 掌握 headers中携带cookie
- 掌握 cookies参数的使用
- 掌握 cookieJar的转换方法
- 掌握 超时参数timeout的使用
- 掌握 代理ip参数proxies的使用
- 掌握 使用verify参数忽略CA整数
- 掌握 requests模块发送POST请求
- 掌握 利用requests.session进行状态保持

### 1.requests模块介绍

> requests文档

#### 1.1 requestes模块作用：

* 发送http请求，获取响应数据

#### 1.2 requests 是第三方模块，需要在python(虚拟)环境中额外安装

* pip/pip3 install requests

#### 1.3 requests模块发送get请求

> 1.需求：通过requests向百度发送请求，获取页面的源码
> 2.运行下面代码，观察打印输出的结果

```python
# 代码实现
##导入requests模块
import requests

##设置url
url = 'https://www.baidu.com'

## 发送get请求，获取响应
response = requests.get(url)

## 打印响应内容
print(resposne.text)
 
```


### 2.response响应对象

> 上面的代码发现打印出来的好多乱码，这是因为编解码使用了不同的字符集

#### 2.1 response.text 和response.content的区别

- requests.text
	- 类型: str
	- 解码类型： requests模块自动根据http头部对响应的编码作出有根据的推测，推测出的文本编码
- response.content
    	- 类型:bytes
	- 解码类型:没有指定

```python
##导入requests模块
import requests

##设置url
url = 'https://www.baidu.com'

## 发送get请求，获取响应
response = requests.get(url)

## 打印响应内容
#print(resposne.text)
#这种直接打印方式是按照推测出的编码格式打印
# 手动设置response的编码格式,解决编码问题
#response.encoding = 'utf8'
#print(response.text)

```

#### 2.2 通过对response.content进行decode来解决中文乱码

- response.content.decode()	默认utf-8
- response.content.decode('GBK')
- 常见的编码字符集
	- utf-8
	- gbk
	- gb2312
	- ascii
	- iso-8859-1

```python
## 使用response.content
print(response.content.decode('utf8')
```

#### 2.3 response响应对象的其他常用 属性或方法

> response = requests.get(url)中response是发送请求的响应对象；response响应对象中除了text、content获取响应内容以外还有其他常用的属性或方法：

- response.url 响应的url
- response.status_code 响应状态码
- response.request.headers 响应对应的请求头
- response.header 响应头
- response.request_cookies 响应对象请求的cookie;返回coolkieJar类型
- response.cookies 响应的cookie(经过了set-cookie动作；返回cookieJar类型)
- response.json() 自动将json字符串类型的响应内容转换为python对象(dict or list)

```python
# response的其他属性
import requests

# 目标url
url = 'https://www.baidu.com'

# 向目标url发送get请求，并获取响应存储到response
response = requests.get(url)

print(response.url)	#打印响应的url

print(response.status_code)	#打印响应的状态码

print(response.request.headers)		#打印响应对象的请求头

print(response.headers)		#打印响应头

print(response.request._cookies)     # 打印请求携带的cookies

print(response.cookies)     # 打印响应中携带的cookies

```

### 3.requests模块发送请求

#### 3.1 发送带header的请求

> 先写一个获取百度首页的代码

```python

import requests

url = 'https://www.baidu.com'

response = requests.get(url)

print(response.content.decode())

print(response.request.headers)

```

##### 3.1.1 思考
1. 对比浏览器上百度首页的网页源代码和上面代码中获取的百度首页的源码，有什么不同？

    - 查看网页源码的方式：
        - 右键-查看网页源代码
        - 右键-检查
        - <kbd>F12</kbd>
2. 对比对应url的响应内容和代码获取的百度首页的源码，有什么不同？

    -   查看对应url响应内容的方法：
        1. 右键检查
        2. 点击net-work
        3. 勾选preserve log
        4. 刷新页面
        5. 查看Name一栏下和浏览器地址相同url的Response

3. 代码中的百度首页的源码非常少，为什么？

    - 需要我们带上请求头信息
> 爬虫要模拟浏览器，欺骗服务器，获取和浏览器一致的内容
    - 请求头中有很多字段，其中User-Agent字段必不可少，表示客户端的操作系统以及浏览器的信息

##### 3.1.2 携带请求头发送请求的方法

    requests.get(url, headers=headers)
    - headers参数接受字典形式的请求头
    - 请求头字段名为key,字段值为value

##### 3.1.3 完成代码实现

> 从浏览器中复制User-Agent,构造headers字段，完下面的代码

```python
import requests

url = 'https://www.baidu.com'

headers = {'User-Agent':'Mozilla/5.0 (X11; Linux x86_64; rv:79.0) Gecko/20100101 Firefox/79.0'}

response = requests.get(url,headers=headers)

print(response.content.decode())

```

#### 3.2 发送带参数的请求

#### 3.2.1 直接在url中携带参数

```python
...

url='https://www.baidu.com/s?wd=python'

...

```

##### 3.2.2 使用params参数

```python

import requests

url = 'https://www.baidu.com/s?'

parameters = {'wd':'python'}

response = requests.get(url,params=parameters)

print(response.content.decode())
```
